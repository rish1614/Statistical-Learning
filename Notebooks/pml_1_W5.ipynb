{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9409f934",
   "metadata": {},
   "source": [
    "# **Introduction**\n",
    "\n",
    "This notebook provides a comprehensive exploration of **univariate probabilistic models**, focusing on foundational concepts of the Probabilistic Machine Learning . It is designed to build strong conceptual clarity on how randomness and uncertainty are quantified and modeled using probability theory. The notebook begins by explaining the core ideas of **random variables**, both **discrete** and **continuous**, including how their behaviors are described through **probability mass functions (pmf)** and **probability density functions (pdf)**.\n",
    "\n",
    "Key statistical tools such as the **cumulative distribution function (cdf)** and **quantile functions** are introduced, with intuitive explanations and real-world examples to illustrate how these functions help characterize probability distributions. The notebook also discusses the **moments of a distribution**, including **mean**, **variance**, **standard deviation**, and extends into **conditional moments**, supported by derivations and visualization.\n",
    "\n",
    "A major part of the notebook emphasizes the distinction between **epistemic (model) uncertainty** and **aleatoric (data) uncertainty**, helping build a foundation for understanding variability in data and predictions. Concepts of **independence** and **conditional independence** between random variables are covered, along with the **law of total expectation** and **law of total variance**, using both mathematical derivations and practical analogies.\n",
    "\n",
    "The final section highlights the **limitations of summary statistics** using famous examples like **Anscombe’s quartet** and the **Datasaurus Dozen**, demonstrating how datasets with identical low-order statistics can still be fundamentally different in structure and distribution.\n",
    "\n",
    "Throughout the notebook, the explanations are supported with **mathematical proofs** making it a complete self-contained study guide on univariate models and their real-world implications in probabilistic modeling.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d1c6ed",
   "metadata": {},
   "source": [
    "# Univariate Model\n",
    "\n",
    "## Definition:\n",
    "A univariate model is a model that uses only one input (independent) variable to predict or explain one output (dependent) variable.\n",
    "\n",
    "## Key Properties:\n",
    "- Uses only 1 feature (e.g., income, age, temperature).\n",
    "- Common in simple regression or time series analysis.\n",
    "- Easy to interpret and visualize (e.g., line plot or scatter plot).\n",
    "\n",
    "## Example:\n",
    "Linear regression with one feature:\n",
    "    Price = β₀ + β₁ × SquareFeet + ε\n",
    "\n",
    "## Use Cases:\n",
    "- Exploratory analysis\n",
    "- Simple prediction problems\n",
    "- Time series forecasting (e.g., AR models)\n",
    "\n",
    "## Limitations:\n",
    "- Ignores other potential influencing variables\n",
    "- Not suitable for complex real-world problems\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5901a21",
   "metadata": {},
   "source": [
    "# Probability: Concepts & Interpretations\n",
    "\n",
    "## What is Probability?\n",
    "Probability is the mathematical study of uncertainty. It quantifies how likely an event is to occur, especially when the outcome is not certain.\n",
    "\n",
    "## Two Interpretations of Probability\n",
    "\n",
    "1. Frequentist Interpretation:\n",
    "- Defines probability as the long-run frequency of an event.\n",
    "- Assumes repeated trials under identical conditions.\n",
    "- Example: In 1000 flips of a fair coin, we expect about 500 heads.\n",
    "- Limitation: Cannot reason about unique or one-time events.\n",
    "\n",
    "2. Bayesian Interpretation:\n",
    "- Defines probability as a measure of belief or uncertainty.\n",
    "- Based on prior knowledge, updated by evidence.\n",
    "- Example: Probability that the ice caps will melt by 2030.\n",
    "- Can model one-time events and adapt based on new data.\n",
    "\n",
    "## Why Bayesian is Used in Machine Learning:\n",
    "- Captures model uncertainty\n",
    "- Useful for rare/unique cases\n",
    "- Naturally aligns with decision-making under uncertainty\n",
    "- Allows probability updating as new data becomes available\n",
    "\n",
    "## Key Point:\n",
    "Both interpretations use the same core probability rules (e.g., Bayes' Theorem, Conditional Probability), even though they interpret them differently.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d1dc584",
   "metadata": {},
   "source": [
    "# Types of Uncertainty\n",
    "\n",
    "## 1. Epistemic Uncertainty (Model Uncertainty):\n",
    "- Caused by a lack of knowledge or incomplete models.\n",
    "- Can be reduced by collecting more or better-quality data.\n",
    "- Example: Not knowing all variables that affect stock prices.\n",
    "\n",
    "## 2. Aleatoric Uncertainty (Data/Noise Uncertainty):\n",
    "- Caused by inherent randomness or noise in the system.\n",
    "- Cannot be reduced even with more data.\n",
    "- Example: Tossing a fair coin (p = 0.5 for heads or tails).\n",
    "\n",
    "## Importance in Machine Learning:\n",
    "- In active learning, we select samples with high uncertainty (high entropy).\n",
    "- If uncertainty is epistemic → collect more data is helpful.\n",
    "- If uncertainty is aleatoric → collecting more data doesn’t help.\n",
    "\n",
    "## Tip:\n",
    "- Epistemic = ignorance (fixable).\n",
    "- Aleatoric = randomness (unfixable).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828e44fd",
   "metadata": {},
   "source": [
    "# Probability Rules\n",
    "\n",
    "## 1. Conjunction (AND / Joint Probability)\n",
    "- Pr(A ∧ B) = Pr(A, B)\n",
    "- If A and B are independent: Pr(A, B) = Pr(A) × Pr(B)\n",
    "\n",
    "## 2. Union (OR Probability)\n",
    "- Pr(A ∨ B) = Pr(A) + Pr(B) − Pr(A ∧ B)\n",
    "- If A and B are mutually exclusive: Pr(A ∨ B) = Pr(A) + Pr(B)\n",
    "\n",
    "## 3. Conditional Probability\n",
    "- Pr(B | A) = Pr(A ∧ B) / Pr(A)\n",
    "- Not defined if Pr(A) = 0\n",
    "\n",
    "## 4. Independence\n",
    "- A and B are independent if: Pr(A, B) = Pr(A) × Pr(B)\n",
    "\n",
    "## 5. Conditional Independence\n",
    "- A and B are conditionally independent given C:\n",
    "  Pr(A, B | C) = Pr(A | C) × Pr(B | C)\n",
    "- Notation: A ⊥ B | C\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06141764",
   "metadata": {},
   "source": [
    "# Random Variables\n",
    "\n",
    "## What is a Random Variable?\n",
    "- A variable whose value is uncertain and depends on chance.\n",
    "- Sample Space: The set of all possible values the variable can take.\n",
    "\n",
    "## Discrete Random Variables\n",
    "- Values are countable (e.g., dice rolls, coin flips).\n",
    "- Probability Mass Function (pmf): \n",
    "  - p(x) = Pr(X = x)\n",
    "  - 0 ≤ p(x) ≤ 1\n",
    "  - Sum over all p(x) = 1\n",
    "\n",
    "### Examples:\n",
    "- Fair die: p(x) = 1/6 for x in {1,2,3,4,5,6}\n",
    "- Degenerate: p(x) = 1 if x = 1, else 0\n",
    "\n",
    "## Continuous Random Variables\n",
    "- Can take any value in ℝ (e.g., height, temperature)\n",
    "- Individual values have zero probability; use intervals.\n",
    "\n",
    "### Cumulative Distribution Function (CDF)\n",
    "- Notation: P(x) = Pr(X ≤ x)\n",
    "- Monotonically non-decreasing\n",
    "- Pr(a < X ≤ b) = P(b) − P(a)\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "- p(x) = d/dx P(x)\n",
    "- Pr(a < X ≤ b) = ∫ from a to b of p(x) dx\n",
    "- For small dx: Pr(x < X ≤ x + dx) ≈ p(x) × dx\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65485612",
   "metadata": {},
   "source": [
    "# Central Limit Theorem (CLT)\n",
    "\n",
    "## Statement:\n",
    "The CLT states that, regardless of the original population distribution,\n",
    "the distribution of the sample means will approach a normal distribution\n",
    "as the sample size increases.\n",
    "\n",
    "## Conditions:\n",
    "- Sample size n ≥ 30 (larger is better)\n",
    "- Samples must be independent and identically distributed\n",
    "- Population variance must be finite\n",
    "\n",
    "## Mathematical Form:\n",
    "If X₁, X₂, ..., Xn are i.i.d. with mean μ and variance σ²:\n",
    "Then as n → ∞,\n",
    "  Z = (X̄ - μ) / (σ / √n) → N(0, 1)\n",
    "\n",
    "## Real-Life Example:\n",
    "Delivery times (skewed) averaged over days → becomes bell-shaped\n",
    "\n",
    "## Why It Matters:\n",
    "- Justifies using normal-based methods even when data is not normal\n",
    "- Forms the basis of confidence intervals, hypothesis testing, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc5776b",
   "metadata": {},
   "source": [
    "# Central Limit Theorem (CLT) Summary with Example\n",
    "\n",
    "Given:\n",
    "- Population Mean (μ) = 15\n",
    "- Population Std. Dev (σ) = 4\n",
    "- Sample Size (n) = 25\n",
    "\n",
    "Then:\n",
    "- Standard Error (SE) = σ / √n = 4 / √25 = 0.8\n",
    "- The sample means (averages of size-25 samples) will follow a normal distribution:\n",
    "    ~ N(μ, σ²/n) = N(15, 0.64)\n",
    "\n",
    "This is true even if the original data is not normally distributed!\n",
    "CLT guarantees that the distribution of sample means becomes normal\n",
    "as n increases (rule of thumb: n ≥ 30).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfd1813",
   "metadata": {},
   "source": [
    "# Normal Distribution\n",
    "\n",
    "## Definition:\n",
    "A probability distribution where values are symmetrically distributed around the mean in a bell-shaped curve.\n",
    "\n",
    "## Formula:\n",
    "f(x) = (1 / sqrt(2πσ²)) * e^(-(x - μ)² / (2σ²))\n",
    "\n",
    "## Parameters:\n",
    "- μ (mu): Mean (center)\n",
    "- σ (sigma): Standard deviation (spread)\n",
    "\n",
    "## Properties:\n",
    "- Symmetric shape\n",
    "- Mean = Median = Mode\n",
    "- Total area under curve = 1\n",
    "- Follows 68-95-99.7 Rule\n",
    "\n",
    "## Central Limit Theorem:\n",
    "The average of a large number of independent samples from any distribution tends to be normally distributed.\n",
    "\n",
    "## Real-Life Examples:\n",
    "- Human heights\n",
    "- Exam scores\n",
    "- Errors in measurements\n",
    "\n",
    "## Applications:\n",
    "- Hypothesis testing\n",
    "- Confidence intervals\n",
    "- Regression assumptions\n",
    "- Data normalization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddf3ae02",
   "metadata": {},
   "source": [
    "# Quantiles\n",
    "\n",
    "## What is a Quantile?\n",
    "- A quantile is the value below which a given percentage of data falls.\n",
    "\n",
    "## Inverse CDF / Quantile Function / Percent Point Function (PPF)\n",
    "- P⁻¹(q) = the value x such that Pr(X ≤ x) = q\n",
    "\n",
    "## Common Quantiles\n",
    "- Median: P⁻¹(0.5) → 50% of data below\n",
    "- Lower Quartile: P⁻¹(0.25)\n",
    "- Upper Quartile: P⁻¹(0.75)\n",
    "- 95% Interval (for Normal): (P⁻¹(0.025), P⁻¹(0.975)) ≈ (−1.96, 1.96)\n",
    "\n",
    "## Example (Standard Normal: N(0,1))\n",
    "- 95% of values fall within (−1.96, 1.96)\n",
    "\n",
    "## General Normal Distribution: N(μ, σ²)\n",
    "- 95% Interval: (μ − 1.96σ, μ + 1.96σ)\n",
    "- Approximate: μ ± 2σ\n",
    "\n",
    "## Real-Life Example\n",
    "- Average height = 170 cm, σ = 10 cm\n",
    "- 95% heights: 170 ± 20 = (150 cm, 190 cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498776f0",
   "metadata": {},
   "source": [
    "# How Quantiles Divide a Probability Distribution\n",
    "\n",
    "## Definition\n",
    "- The q-th quantile, x_q, is the value where:\n",
    "  P(X ≤ x_q) = q\n",
    "- This divides the distribution into two parts:\n",
    "  - Left: q probability mass\n",
    "  - Right: 1 − q probability mass\n",
    "\n",
    "## Special Case: Median\n",
    "- Median = 0.5 quantile\n",
    "- Divides the distribution exactly in half:\n",
    "  - 50% of values ≤ median\n",
    "  - 50% ≥ median\n",
    "\n",
    "## Example (Standard Normal Distribution)\n",
    "- Q1 (25% quantile): ≈ -0.674\n",
    "- Median (50% quantile): 0\n",
    "- Q3 (75% quantile): ≈ +0.674\n",
    "\n",
    "## Visualization\n",
    "- Quantiles are vertical \"cut-points\" on the x-axis of a PDF plot.\n",
    "- Area under curve to the left = cumulative probability = q\n",
    "\n",
    "## Conclusion\n",
    "- Quantiles are useful to summarize data, detect outliers, and understand spread.\n",
    "- Each quantile point divides the distribution into predictable, equal-probability parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18cade5",
   "metadata": {},
   "source": [
    "# How Quantiles Divide a Probability Distribution\n",
    "\n",
    "## Definition\n",
    "- The q-th quantile, x_q, is the value where:\n",
    "  P(X ≤ x_q) = q\n",
    "- This divides the distribution into two parts:\n",
    "  - Left: q probability mass\n",
    "  - Right: 1 − q probability mass\n",
    "\n",
    "## Special Case: Median\n",
    "- Median = 0.5 quantile\n",
    "- Divides the distribution exactly in half:\n",
    "  - 50% of values ≤ median\n",
    "  - 50% ≥ median\n",
    "\n",
    "## Example (Standard Normal Distribution)\n",
    "- Q1 (25% quantile): ≈ -0.674\n",
    "- Median (50% quantile): 0\n",
    "- Q3 (75% quantile): ≈ +0.674\n",
    "\n",
    "## Visualization\n",
    "- Quantiles are vertical \"cut-points\" on the x-axis of a PDF plot.\n",
    "- Area under curve to the left = cumulative probability = q\n",
    "\n",
    "## Conclusion\n",
    "- Quantiles are useful to summarize data, detect outliers, and understand spread.\n",
    "- Each quantile point divides the distribution into predictable, equal-probability parts.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51d724b7",
   "metadata": {},
   "source": [
    "# Sets of Related Random Variables\n",
    "\n",
    "## 1. Joint Distribution\n",
    "- p(X, Y) = probability that both X and Y happen.\n",
    "- Represented in a table (if both variables are discrete).\n",
    "- All values in joint distribution must sum to 1.\n",
    "\n",
    "## 2. Marginal Distribution\n",
    "- Probability of a single variable, ignoring others.\n",
    "- p(X) = ∑ p(X, Y) over all Y (called sum rule).\n",
    "\n",
    "## 3. Conditional Probability\n",
    "- p(Y | X) = p(X, Y) / p(X)\n",
    "- Gives probability of Y given that X has occurred.\n",
    "\n",
    "## 4. Product Rule\n",
    "- p(X, Y) = p(X) * p(Y | X)\n",
    "\n",
    "## 5. Independence\n",
    "- If X ⊥ Y → p(X, Y) = p(X) * p(Y)\n",
    "- Requires fewer parameters to define the model.\n",
    "\n",
    "## 6. Chain Rule (for many variables)\n",
    "- p(x₁, x₂, ..., x_D) = p(x₁) * p(x₂ | x₁) * ... * p(x_D | x₁:₍D−1₎)\n",
    "\n",
    "## Real-Life Examples:\n",
    "- Weather vs Traffic\n",
    "- Dice rolls\n",
    "- Multiday weather predictions\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd19f11",
   "metadata": {},
   "source": [
    "# Independence and Conditional Independence\n",
    "\n",
    "## Independence:\n",
    "- X ⊥ Y  ⇔  p(X, Y) = p(X) * p(Y)\n",
    "- Knowing X doesn't change belief about Y\n",
    "- Real-life: Two dice rolled separately\n",
    "\n",
    "## Mutual Independence:\n",
    "- All subsets of variables are independent\n",
    "- Example: Tossing 3 independent coins\n",
    "\n",
    "## Conditional Independence:\n",
    "- X ⊥ Y | Z  ⇔  p(X, Y | Z) = p(X | Z) * p(Y | Z)\n",
    "- Once Z is known, X and Y become independent\n",
    "- Real-life: \n",
    "  - Weather mediates between Ice Cream & Umbrella\n",
    "  - Disease mediates between Symptoms\n",
    "\n",
    "## Graph Notation:\n",
    "- X — Z — Y: Z mediates the relationship between X and Y\n",
    "- Used in graphical models to simplify complex distributions\n",
    "\n",
    "\n",
    "* Unconditional independence is rare; Conditional independence is more useful in practice.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb82f3ed",
   "metadata": {},
   "source": [
    "# Moments of a Distribution\n",
    "\n",
    "## 1. Mean (Expected Value)\n",
    "\n",
    "### Continuous:\n",
    "$$\n",
    "E[X] = \\int_{-\\infty}^{\\infty} x \\cdot p(x) \\, dx\n",
    "$$\n",
    "\n",
    "### Discrete:\n",
    "$$\n",
    "E[X] = \\sum_{x \\in \\mathcal{X}} x \\cdot p(x)\n",
    "$$\n",
    "\n",
    "### Linearity of Expectation:\n",
    "$$\n",
    "E[aX + b] = aE[X] + b\n",
    "$$\n",
    "$$\n",
    "E\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n E[X_i]\n",
    "$$\n",
    "\n",
    "### If Independent:\n",
    "$$\n",
    "E\\left[\\prod_{i=1}^n X_i\\right] = \\prod_{i=1}^n E[X_i]\n",
    "$$\n",
    "\n",
    "## 2. Variance\n",
    "\n",
    "### Definition:\n",
    "$$\n",
    "\\text{Var}[X] = E[(X - \\mu)^2] = \\int (x - \\mu)^2 p(x) dx\n",
    "$$\n",
    "\n",
    "### Shortcut:\n",
    "$$\n",
    "\\text{Var}[X] = E[X^2] - \\mu^2\n",
    "$$\n",
    "\n",
    "### Standard Deviation:\n",
    "$$\n",
    "\\text{std}[X] = \\sqrt{\\text{Var}[X]} = \\sigma\n",
    "$$\n",
    "\n",
    "### Scaling:\n",
    "$$\n",
    "\\text{Var}[aX + b] = a^2 \\cdot \\text{Var}[X]\n",
    "$$\n",
    "\n",
    "### Variance of Sums (Independent):\n",
    "$$\n",
    "\\text{Var}\\left[\\sum_{i=1}^n X_i\\right] = \\sum_{i=1}^n \\text{Var}[X_i]\n",
    "$$\n",
    "\n",
    "### Variance of Products (Independent):\n",
    "$$\n",
    "\\text{Var}\\left[\\prod_{i=1}^n X_i\\right] = \\prod_i (V[X_i] + (E[X_i])^2) - \\prod_i (E[X_i])^2\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583ae055",
   "metadata": {},
   "source": [
    "# Conditional Moments\n",
    "\n",
    "## Law of Total Expectation:\n",
    "$$\n",
    "\\mathbb{E}[X] = \\mathbb{E}_Y[\\mathbb{E}[X|Y]]\n",
    "$$\n",
    "\n",
    "### Meaning:\n",
    "- Compute expected value of X **within each group Y**\n",
    "- Average those values weighted by \\( P(Y) \\)\n",
    "\n",
    "## Discrete Proof of the Law of Total Expectation\n",
    "\n",
    "We want to prove:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\mathbb{E}_Y[\\mathbb{E}[X|Y]]\n",
    "$$\n",
    "\n",
    "### Step-by-step Derivation\n",
    "\n",
    "Start from the right-hand side:\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_Y[\\mathbb{E}[X|Y]] = \\sum_y \\left( \\sum_x x \\cdot p(X = x \\mid Y = y) \\right) p(Y = y)\n",
    "$$\n",
    "\n",
    "By the definition of conditional probability:\n",
    "\n",
    "$$\n",
    "= \\sum_y \\sum_x x \\cdot p(X = x, Y = y)\n",
    "$$\n",
    "\n",
    "Change the order of summation:\n",
    "\n",
    "$$\n",
    "= \\sum_x \\sum_y x \\cdot p(X = x, Y = y)\n",
    "$$\n",
    "\n",
    "Factor out \\( x \\):\n",
    "\n",
    "$$\n",
    "= \\sum_x x \\cdot \\left( \\sum_y p(X = x, Y = y) \\right)\n",
    "$$\n",
    "\n",
    "Recognize that summing over all \\( y \\) gives the marginal \\( p(X = x) \\):\n",
    "\n",
    "$$\n",
    "= \\sum_x x \\cdot p(X = x)\n",
    "$$\n",
    "\n",
    "Which is by definition:\n",
    "\n",
    "$$\n",
    "= \\mathbb{E}[X]\n",
    "$$\n",
    "\n",
    "## Real-life Example:\n",
    "\n",
    "Let X = lightbulb lifetime, Y = factory\n",
    "\n",
    "Given:\n",
    "- \\( \\mathbb{E}[X|Y=1] = 5000 \\), \\( \\mathbb{E}[X|Y=2] = 4000 \\)\n",
    "- \\( p(Y=1) = 0.6 \\), \\( p(Y=2) = 0.4 \\)\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\mathbb{E}[X] = 0.6 \\cdot 5000 + 0.4 \\cdot 4000 = 4600\n",
    "$$\n",
    "\n",
    "## Law of Total Variance – Derivation\n",
    "\n",
    "We want to prove:\n",
    "\n",
    "$$\n",
    "\\text{Var}[X] = \\mathbb{E}_Y[\\text{Var}[X|Y]] + \\text{Var}_Y[\\mathbb{E}[X|Y]]\n",
    "$$\n",
    "\n",
    "### Step 1: Start with variance definition\n",
    "\n",
    "$$\n",
    "\\text{Var}[X] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2\n",
    "$$\n",
    "\n",
    "### Step 2: Apply law of total expectation\n",
    "\n",
    "- For the first term:\n",
    "  $$\n",
    "  \\mathbb{E}[X^2] = \\mathbb{E}_Y[\\mathbb{E}[X^2|Y]]\n",
    "  $$\n",
    "- For the second term:\n",
    "  $$\n",
    "  \\mathbb{E}[X] = \\mathbb{E}_Y[\\mathbb{E}[X|Y]] \\Rightarrow (\\mathbb{E}[X])^2 = \\left( \\mathbb{E}_Y[\\mathbb{E}[X|Y]] \\right)^2\n",
    "  $$\n",
    "\n",
    "### Step 3: Plug into the variance formula\n",
    "\n",
    "$$\n",
    "\\text{Var}[X] = \\mathbb{E}_Y[\\mathbb{E}[X^2|Y]] - \\left( \\mathbb{E}_Y[\\mathbb{E}[X|Y]] \\right)^2\n",
    "$$\n",
    "\n",
    "### Step 4: Add and subtract middle term\n",
    "\n",
    "Add and subtract \\( \\mathbb{E}_Y[\\mathbb{E}[X|Y]^2] \\) to split the expression:\n",
    "\n",
    "$$\n",
    "\\text{Var}[X] = \\underbrace{\\mathbb{E}_Y[\\mathbb{E}[X^2|Y] - \\mathbb{E}[X|Y]^2]}_{\\text{(1) Expected conditional variance}} + \\underbrace{\\mathbb{E}_Y[\\mathbb{E}[X|Y]^2] - \\left( \\mathbb{E}_Y[\\mathbb{E}[X|Y]] \\right)^2}_{\\text{(2) Variance of conditional expectation}}\n",
    "$$\n",
    "\n",
    "### Final Result\n",
    "\n",
    "$$\n",
    "\\boxed{\n",
    "\\text{Var}[X] = \\mathbb{E}_Y[\\text{Var}[X|Y]] + \\text{Var}_Y[\\mathbb{E}[X|Y]]\n",
    "}\n",
    "$$\n",
    "\n",
    "### Intuition:\n",
    "\n",
    "- First term: average variance within each group (given \\( Y \\))\n",
    "- Second term: variance between group means (how much \\( \\mathbb{E}[X|Y] \\) varies)\n",
    "\n",
    "\n",
    "## Real-Life Example: Lightbulb Lifetimes from Two Factories\n",
    "\n",
    "Let:  \n",
    "- $X$: Lifetime of a lightbulb  \n",
    "- $Y$: Factory from which the lightbulb comes\n",
    "\n",
    "Assume:\n",
    "\n",
    "- **Factory A** ($Y = A$):  \n",
    "  - Produces 60% of the lightbulbs: $P(Y = A) = 0.6$  \n",
    "  - Mean lifetime: $\\mu_A = \\mathbb{E}[X \\mid Y = A] = 5000$  \n",
    "  - Variance: $\\sigma_A^2 = \\text{Var}(X \\mid Y = A) = 100^2 = 10000$\n",
    "\n",
    "- **Factory B** ($Y = B$):  \n",
    "  - Produces 40% of the lightbulbs: $P(Y = B) = 0.4$  \n",
    "  - Mean lifetime: $\\mu_B = \\mathbb{E}[X \\mid Y = B] = 4000$  \n",
    "  - Variance: $\\sigma_B^2 = \\text{Var}(X \\mid Y = B) = 200^2 = 40000$\n",
    "\n",
    "### Step 1: Law of Total Expectation\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = \\mathbb{E}[X \\mid Y = A] \\cdot P(Y = A) + \\mathbb{E}[X \\mid Y = B] \\cdot P(Y = B)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbb{E}[X] = 5000 \\cdot 0.6 + 4000 \\cdot 0.4 = 3000 + 1600 = \\boxed{4600}\n",
    "$$\n",
    "\n",
    "The expected lifetime of a randomly selected lightbulb is **4600 hours**.\n",
    "\n",
    "\n",
    "### Step 2: Law of Total Variance\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = \\mathbb{E}_Y[\\text{Var}(X \\mid Y)] + \\text{Var}_Y[\\mathbb{E}[X \\mid Y]]\n",
    "$$\n",
    "\n",
    "\n",
    "#### First Term: Expected Conditional Variance\n",
    "\n",
    "$$\n",
    "\\mathbb{E}_Y[\\text{Var}(X \\mid Y)] = \\sigma_A^2 \\cdot P(Y = A) + \\sigma_B^2 \\cdot P(Y = B)\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 10000 \\cdot 0.6 + 40000 \\cdot 0.4 = 6000 + 16000 = \\boxed{22000}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Second Term: Variance of Conditional Means\n",
    "\n",
    "Let $\\mu = \\mathbb{E}[X] = 4600$. Then:\n",
    "\n",
    "$$\n",
    "\\text{Var}_Y[\\mathbb{E}[X \\mid Y]] = P(Y = A)(\\mu_A - \\mu)^2 + P(Y = B)(\\mu_B - \\mu)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.6 \\cdot (5000 - 4600)^2 + 0.4 \\cdot (4000 - 4600)^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "= 0.6 \\cdot 160000 + 0.4 \\cdot 360000 = 96000 + 144000 = \\boxed{240000}\n",
    "$$\n",
    "\n",
    "### Final Answer: Total Variance\n",
    "\n",
    "$$\n",
    "\\text{Var}(X) = 22000 + 240000 = \\boxed{262000}\n",
    "$$\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "- $\\mathbb{E}[X] = 4600$ hours  \n",
    "- $\\text{Var}(X) = 262000$ hours²\n",
    "\n",
    "This shows how total variability arises from both:  \n",
    "1. **Variation within each factory** (random noise), and  \n",
    "2. **Variation between factories** (systematic difference in means).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7eea185",
   "metadata": {},
   "source": [
    "# Limitations of Summary Statistics\n",
    "\n",
    "## Objective\n",
    "\n",
    "The objective of this section is to understand why relying solely on summary statistics such as the mean, variance, and correlation can be misleading in data analysis. Through well-known examples such as Anscombe’s Quartet and the Datasaurus Dozen, we will demonstrate that datasets with identical summary statistics can have drastically different underlying distributions. This highlights the importance of combining statistical summaries with graphical data visualization techniques to fully understand the characteristics of data.\n",
    "\n",
    "## Why Summary Statistics Are Used\n",
    "\n",
    "Summary statistics are numerical measures that describe and summarize features of a dataset. Common statistics include:\n",
    "\n",
    "- Mean (average): Indicates the central tendency.\n",
    "- Variance: Measures the spread or variability of the data.\n",
    "- Correlation coefficient: Measures the strength and direction of a linear relationship between two variables.\n",
    "\n",
    "These summaries are often used because they are concise and easy to compute, providing a quick overview of the data. However, they can obscure deeper patterns, anomalies, or structures present in the dataset.\n",
    "\n",
    "## Case Study 1: Anscombe’s Quartet\n",
    "\n",
    "In 1973, statistician Francis Anscombe created four small datasets of (x, y) pairs. Each dataset contains 11 data points, and all four have:\n",
    "\n",
    "- Mean of x: E[x] = 9\n",
    "- Variance of x: Var[x] = 11\n",
    "- Mean of y: E[y] = 7.5\n",
    "- Variance of y: Var[y] = 4.12\n",
    "- Correlation between x and y: ρ(x, y) = 0.816\n",
    "\n",
    "Despite having identical summary statistics, these four datasets are fundamentally different in structure. For example:\n",
    "\n",
    "- Dataset I exhibits a linear relationship.\n",
    "- Dataset II displays a clear non-linear relationship.\n",
    "- Dataset III includes an outlier that strongly influences the correlation.\n",
    "- Dataset IV has most data points aligned vertically with one distant outlier.\n",
    "\n",
    "These differences become immediately obvious when visualizing the data, but remain hidden if one only examines the statistics.\n",
    "\n",
    "## Case Study 2: The Datasaurus Dozen\n",
    "\n",
    "A more recent and visually striking demonstration of the same issue is the Datasaurus Dozen, a set of 12 datasets that all share the same low-order statistical properties, including:\n",
    "\n",
    "- Identical means and variances of x and y\n",
    "- Identical correlation coefficients\n",
    "\n",
    "However, the shape and distribution of the data vary dramatically. The datasets include:\n",
    "\n",
    "- A dinosaur-shaped scatter plot (the original \"Datasaurus\")\n",
    "- Various geometric shapes like a circle, star, and bullseye\n",
    "- Patterns such as slanted lines, dots, vertical and horizontal bars\n",
    "\n",
    "These datasets were generated using simulated annealing, an optimization technique that slightly modifies data points to preserve the original summary statistics while reshaping the data into visually distinct patterns. This process confirms that it is possible to engineer datasets with identical statistics but vastly different distributions.\n",
    "\n",
    "## Real-Life Example: Factory Production Stability\n",
    "\n",
    "Consider two machines in a manufacturing plant:\n",
    "\n",
    "- Machine A produces 1000 units per day, with very little variation (e.g., values between 990 and 1010).\n",
    "- Machine B also has an average daily output of 1000 units, but with high variability (e.g., sometimes 500 units, sometimes 1500).\n",
    "\n",
    "Both machines have the same mean output. They could also be configured to have the same variance. But in practice:\n",
    "\n",
    "- Machine A is reliable and predictable.\n",
    "- Machine B is unstable and could lead to supply chain issues.\n",
    "\n",
    "Without looking at the full distribution of production values or visualizing a time series, the summary statistics alone fail to reveal the operational differences between the two machines.\n",
    "\n",
    "## Better Visualization Tools\n",
    "\n",
    "To obtain a complete understanding of a dataset, it is essential to visualize the data using appropriate graphical techniques. Some commonly used visualization methods include:\n",
    "\n",
    "- **Scatter plots**: Reveal relationships, clusters, and shapes in bivariate data.\n",
    "- **Box plots**: Summarize distributions while identifying outliers and spread.\n",
    "- **Violin plots**: Display the full distribution along with summary statistics.\n",
    "- **Density plots**: Provide a smooth estimation of data distribution.\n",
    "- **Time series plots**: Capture patterns, trends, and irregularities over time.\n",
    "\n",
    "These tools complement statistical summaries by revealing aspects of the data that numbers alone cannot capture.\n",
    "\n",
    "## Boolean Summary Table\n",
    "\n",
    "The following table summarizes which properties are captured by summary statistics and which are better revealed through visualizations:\n",
    "\n",
    "| Feature                        | Captured by Summary Stats | Captured by Visualization |\n",
    "|-------------------------------|----------------------------|----------------------------|\n",
    "| Mean                          | Yes                        | Yes                        |\n",
    "| Variance                      | Yes                        | Yes                        |\n",
    "| Outliers                      | No                         | Yes                        |\n",
    "| Non-linearity                 | No                         | Yes                        |\n",
    "| Clustering                    | No                         | Yes                        |\n",
    "| Distribution shape            | No                         | Yes                        |\n",
    "| Patterns and symmetry         | No                         | Yes                        |\n",
    "\n",
    "# **Conclusion**\n",
    "\n",
    "Summary statistics such as mean, variance, and correlation are useful tools for summarizing data, but they do not tell the whole story. Two datasets may share identical summary statistics yet differ fundamentally in their shape, distribution, and structure. As shown by Anscombe’s Quartet and the Datasaurus Dozen, data visualization is essential for discovering hidden patterns, outliers, and relationships that statistics may conceal.\n",
    "\n",
    "In practical scenarios, such as evaluating machinery, student performance, or financial markets, reliance on numerical summaries alone can lead to incorrect conclusions. Data visualization allows for deeper insights, improving decision-making and preventing misleading interpretations.\n",
    "\n",
    "Therefore, the key takeaway is:\n",
    "\n",
    "**Always visualize data. Never rely solely on summary statistics.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23ea10a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
