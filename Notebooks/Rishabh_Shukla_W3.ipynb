{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "93433c3a",
   "metadata": {},
   "source": [
    "# Introduction:\n",
    "\n",
    "In this week, I am going to learn how to select the best model for a given dataset from all perspectives using regularisation techniques. After that, I will move on to feature engineering, and then I will learn about cross-validation theory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a28cd6ef",
   "metadata": {},
   "source": [
    "## Model Selection\n",
    "\n",
    "* In this part, I learned what model selection actually means — it's not just picking the most accurate model, but the one that performs well on new data. The main idea is to find a model that balances between underfitting and overfitting.\n",
    "* Too simple models don't learn the patterns (underfitting), and too complex ones start memorizing the data (overfitting). So, model selection helps to figure out what level of complexity is just right.\n",
    "* We also looked at different ways to select models, like subset selection, shrinkage methods (Ridge, Lasso), and using cross-validation for reliable evaluation.\n",
    "\n",
    "The goal is to build a model that’s accurate, but also generalises well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "844a7133",
   "metadata": {},
   "source": [
    "## Prediction Accuracy & Model Interpretability\n",
    "\n",
    "### Prediction Accuracy\n",
    "\n",
    "- Least Squares works well when:\n",
    "  - Relationship is linear\n",
    "  - \\( n >> p \\) (more observations than features) low bias,low varience good for test data also.\n",
    "- Issues arise when:\n",
    "  - \\( n ~~ p \\): High variance → overfitting,poor prediction on test data.\n",
    "  - \\( p > n \\): Infinite solutions, perfect fit on training data but poor on test data\n",
    "- Solution:\n",
    "  - Apply shrinkage (e.g., Ridge/Lasso)\n",
    "  - Reduces variance significantly\n",
    "  - Slight increase in bias, but much better test prediction\n",
    "\n",
    "### Model Interpretability\n",
    "\n",
    "- Many features may not be related to the response. so remove them from model\n",
    "- Including them adds **unnecessary complexity**.\n",
    "- Least Squares doesn’t give zero coefficients → doesn't eliminate useless variables.\n",
    "- Use feature selection methods (like Lasso or subset selection) to:\n",
    "  - Automatically drop irrelevant features\n",
    "  - Improve interpretability\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c135f78",
   "metadata": {},
   "source": [
    "## Methods of Model Selection :\n",
    "\n",
    "We have 3 main alternatives when least squares is not working well:\n",
    "\n",
    "### 1. Subset Selection\n",
    "* Choose only relevant features, drop useless ones\n",
    "* Then apply least squares on that subset\n",
    "\n",
    "**Pros:**\n",
    "* Simple model\n",
    "* Easy to explain and interpret\n",
    "\n",
    "**Cons:**\n",
    "* Finding best subset is slow and computationally expensive\n",
    "* May miss best combination of features\n",
    "\n",
    "### 2. Shrinkage (Regularization)\n",
    "\n",
    "* Use all features, but shrink the coefficients\n",
    "* Methods include:\n",
    "  * Ridge: shrinks all coefficients\n",
    "  * Lasso: shrinks and sets some coefficients to zero\n",
    "\n",
    "**Pros:**\n",
    "* Prevents overfitting\n",
    "* Can perform variable selection (Lasso)\n",
    "* Works well when number of features is large\n",
    "\n",
    "**Cons:**\n",
    "* Introduces bias\n",
    "* Interpretation of model is less straightforward\n",
    "\n",
    "### 3. Dimension Reduction\n",
    "\n",
    "* Combine original variables into fewer components (e.g., using PCA)\n",
    "* Use these new variables in regression\n",
    "\n",
    "**Pros:**\n",
    "* Reduces model complexity\n",
    "* Handles multicollinearity among features\n",
    "\n",
    "**Cons:**\n",
    "* Hard to interpret new variables\n",
    "* Original meaning of features is lost\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa2606c4",
   "metadata": {},
   "source": [
    "# Subset Selection :\n",
    "There are some parts of subset selection methods.\n",
    "## 1.Best - Subset Selection :\n",
    "### What is it?\n",
    "Best Subset Selection is a method where we try every possible combination of predictors (features)  \n",
    "and select the one that gives the best performance.  \n",
    "It is a complete search of all 2^p possible models, where p is the number of predictors.\n",
    "\n",
    "### Steps:\n",
    "**Step 1:** Start with the null model (no predictors).  \n",
    "It only predicts the mean of the response variable.\n",
    "\n",
    "**Step 2:** For k = 1 to p:  \n",
    "- Try all models that use exactly k predictors  \n",
    "- From these, choose the model with the lowest RSS or highest R²  \n",
    "- This gives the best model Mk for each subset size\n",
    "\n",
    "**Step 3:** Choose the final best model from M0 to Mp using:  \n",
    "- Validation error  \n",
    "- Cp (AIC)  \n",
    "- BIC  \n",
    "- Adjusted R²  \n",
    "- Cross-validation  \n",
    "\n",
    "These help to avoid overfitting and focus on test accuracy, not just training accuracy.\n",
    "\n",
    "### Example\n",
    "If we have 4 predictors: A, B, C, D  \n",
    "Then all combinations are tried: A, B, C, D, AB, AC, AD, BC, BD, CD, ABC, ABD, etc.  \n",
    "Total = 2^4 = 16 models\n",
    "\n",
    "### Important Terms\n",
    "- Null model: a model with no predictors, only predicts the mean\n",
    "- RSS: residual sum of squares (lower is better)\n",
    "- R²: tells how well the model explains the output (higher is better)\n",
    "- Cp, AIC, BIC, Adjusted R²: used to select final model based on test error\n",
    "- Cross-validation: tests how well the model performs on new data\n",
    "\n",
    "### Pros\n",
    "- Tries all combinations of predictors  \n",
    "- Finds best model based on training performance  \n",
    "- Easy to understand\n",
    "\n",
    "### Cons\n",
    "- Very slow when number of predictors is large  \n",
    "- Total models = 2^p  \n",
    "  For p = 10 → 1024 models  \n",
    "  For p = 20 → over 1 million models  \n",
    "- Can overfit if final model is selected only based on training RSS or R²\n",
    "\n",
    "### When to use\n",
    "- Use when number of predictors is small (like p ≤ 10)  \n",
    "- Not suitable for large p  \n",
    "- For large p, use stepwise or shrinkage methods"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae863ac7",
   "metadata": {},
   "source": [
    "## 2. Stepwise Selection :\n",
    "#### Why Stepwise Selection?\n",
    "* When the number of predictors (p) becomes large, Best Subset Selection becomes:\n",
    "    * Too slow (computational problem)\n",
    "    * More likely to overfit (statistical problem)\n",
    "* This is because:\n",
    "    * Best Subset Selection tries 2^p models\n",
    "    * Large search space = higher chance of fitting patterns that do not generalize to test data\n",
    "* So we use **Stepwise Selection** to solve these issues:\n",
    "    * It explores fewer models\n",
    "    * It is faster and more practical\n",
    "    * It reduces overfitting risk\n",
    "* There are three main types:\n",
    "    1. **Forward Stepwise Selection**\n",
    "    2. **Backward Stepwise Selection**\n",
    "    3. **Hybrid Stepwise Selection**\n",
    "\n",
    "## a. Forward Stepwise Selection\n",
    "\n",
    "### Definition:\n",
    "A stepwise method that starts from a model with **no predictors**,  \n",
    "and adds one predictor at a time, selecting the one that improves the model the most.\n",
    "\n",
    "### Algorithm Steps :\n",
    "1. Start with the null model (no predictors)  \n",
    "2. For **k = 0** to **p - 1**:\n",
    "   - Check all **p - k** predictors not yet used\n",
    "   - Add each one to current model, fit new models\n",
    "   - Select the one with **lowest RSS** or **highest R²**\n",
    "   - Add it to form new model **Mk+1**\n",
    "3. Select the final model using:\n",
    "   - Validation error  \n",
    "   - AIC (Akaike Information Criterion)  \n",
    "   - BIC (Bayesian Information Criterion)  \n",
    "   - Adjusted R²  \n",
    "   - Cross-validation\n",
    "\n",
    "### Example:\n",
    "If you have predictors A, B, C, D:\n",
    "- Start with none  \n",
    "- Try A, B, C, D → pick best  \n",
    "- Next round: try AB, AC, AD, BC, etc.  \n",
    "- Repeat till no further improvement\n",
    "\n",
    "### Time Advantage:\n",
    "- Best Subset: 2^p models  \n",
    "- Forward Stepwise: only **1 + p(p + 1)/2** models  \n",
    "  For p = 20 → only 211 models (vs over 1 million)\n",
    "\n",
    "### Important Terms:\n",
    "- Null Model: A model with no predictors; only predicts the mean  \n",
    "- RSS (Residual Sum of Squares): Measures training error; lower is better  \n",
    "- R²: Measures how much variance the model explains; higher is better  \n",
    "- Adjusted R² / AIC / BIC: Used to evaluate and compare models  \n",
    "- Cross-validation: Used to estimate test error\n",
    "\n",
    "### Pros:\n",
    "- Much faster than best subset  \n",
    "- Works when **p > n**  \n",
    "- Easy to understand and apply\n",
    "\n",
    "### Cons:\n",
    "- Greedy method — once a predictor is added, it can't be removed  \n",
    "- May miss best overall best model\n",
    "\n",
    "## b. Backward Stepwise Selection\n",
    "\n",
    "### Definition:\n",
    "\n",
    "A stepwise method that starts with a **full model** (all predictors),  \n",
    "and removes one predictor at a time, selecting the least useful one to remove.\n",
    "\n",
    "### Algorithm Steps: \n",
    "1. Start with the full model (all predictors included)  \n",
    "2. For **k = p** down to **1**:\n",
    "   - Consider all **k** models that remove one predictor\n",
    "   - Choose the one with **lowest RSS** or **highest R²**\n",
    "   - This becomes model **Mk-1**\n",
    "3. Choose the final model using:\n",
    "   - AIC, BIC, Adjusted R², or Cross-validation\n",
    "\n",
    "### Important Requirement:\n",
    "- Backward Stepwise needs the **full model to be fit** using least squares  \n",
    "- So it requires **n > p** \n",
    "- Cannot be used when **p > n**\n",
    "\n",
    "### Pros:\n",
    "- Also faster than Best Subset Selection  \n",
    "- Good for datasets where **n > p**\n",
    "\n",
    "### Cons:\n",
    "- Cannot work when **p ≥ n**  \n",
    "- Greedy like forward method — once removed, a predictor can't be added back  \n",
    "- May miss the best model\n",
    "\n",
    "## c. Hybrid Stepwise Selection\n",
    "\n",
    "### Definition:\n",
    "\n",
    "A method that combines forward and backward steps:  \n",
    "- Start with no predictors  \n",
    "- Add predictors one by one (like forward)  \n",
    "- After each addition, check if any previous predictors can now be removed (means if add new predictor and it gives the low accuracy)\n",
    "  - If yes, remove them\n",
    "\n",
    "### Goal:\n",
    "To get closer to Best Subset Selection while keeping speed advantages  \n",
    "of Stepwise methods.\n",
    "\n",
    "## Comparison Table\n",
    "\n",
    "| Feature                     | Best Subset     | Forward Stepwise | Backward Stepwise | Hybrid Stepwise |\n",
    "|-----------------------------|-----------------|------------------|-------------------|-----------------|\n",
    "| Search Type                | All combinations| Adds one at a time| Removes one at a time| Adds + Removes |\n",
    "| Speed                      | Very slow       | Fast             | Fast              | Fast            |\n",
    "| Works with p > n?          | No              | Yes              | No                | Yes             |\n",
    "| Can remove added predictors?| Yes             | No               | Yes               | Yes             |\n",
    "| Model Quality              | Best (on training) | Good           | Good              | Better          |\n",
    "\n",
    "## Summary:\n",
    "- Best Subset is most complete, but too slow for large **p**  \n",
    "- Forward Stepwise is fast, works for **p > n**, but greedy may miss the global optimal model \n",
    "- Backward Stepwise is fast but only works when **n > p**  \n",
    "- Hybrid gives flexibility by allowing both adding and removing predictors\n",
    "\n",
    "Use Stepwise Selection when:\n",
    "- You want a fast model selection  \n",
    "- You have a large number of predictors  \n",
    "- You want a balance between accuracy and efficiency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e80b8c59",
   "metadata": {},
   "source": [
    "## Training Error vs Test Error in Regression Models : \n",
    "In regression, if we add more predictors in the model, then training RSS decreases and R² increases.  \n",
    "That means training error always goes down as we add more variables.  \n",
    "But this does not mean that test error will also reduce.\n",
    "\n",
    "When we add too many predictors, the model starts to overfit the training data.  \n",
    "It learns noise or patterns that are only present in the training set, not in the real-world test data.  \n",
    "This causes the test error to increase even if training error is very low.\n",
    "\n",
    "So we cannot select the best model just by looking at training RSS or R²,  \n",
    "because the full model (with all predictors) will always have the lowest RSS and highest R² on training data.  \n",
    "But that model may perform badly on test data.\n",
    "\n",
    "### How to estimate test error ?\n",
    "Subset selection methods like best subset, forward selection, and backward selection  \n",
    "give us a set of different models with different numbers of predictors.  \n",
    "Now we need a way to choose the best model among them based on test error, not training error.\n",
    "\n",
    "There are two ways to do this:\n",
    "\n",
    "### 1. Indirect estimation of test error : \n",
    "We can adjust the training error by applying a penalty for model complexity.  \n",
    "This helps to avoid overfitting.\n",
    "\n",
    "Common metrics:\n",
    "- Adjusted R²  \n",
    "- AIC (Akaike Information Criterion)  \n",
    "- BIC (Bayesian Information Criterion)  \n",
    "- Cp statistic  \n",
    "\n",
    "These metrics reduce the score if extra predictors are added without actual improvement.\n",
    "\n",
    "### 2. Direct estimation of test error : \n",
    "We can estimate test error directly using:\n",
    "- Validation set approach  \n",
    "- Cross-validation (like k-fold CV)\n",
    "\n",
    "* In validation set, we split the data into training and test set.  \n",
    "* In cross-validation, we divide the data into multiple parts and train/test the model multiple times.  \n",
    "This gives a better estimate of how the model performs on unseen data.\n",
    "\n",
    "### Conclusion\n",
    "- Training error is not a good estimate of test error.  \n",
    "- RSS and R² are only useful for training error.  \n",
    "- Use adjusted metrics or cross-validation to estimate test error.  \n",
    "- Always select the model which gives the lowest test error, not just the lowest training error.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17a35044",
   "metadata": {},
   "source": [
    "## Common Metrics Evaluation for Selecting best model based on test error.\n",
    "## Cp Statistic (Mallow's Cp) :\n",
    "In regression, training RSS always goes down when we add more predictors.  \n",
    "But test error may go up due to overfitting.  \n",
    "So training RSS is not a good estimate of test error.\n",
    "\n",
    "Cp statistic is used to estimate test error by adjusting the training RSS.\n",
    "\n",
    "### Cp Formula:\n",
    "Cp = (1/n) * [ RSS + 2 * d * σ²_hat ]\n",
    "\n",
    "Where:\n",
    "- RSS = training residual sum of squares  \n",
    "- d = number of predictors in the model  \n",
    "- σ²_hat = estimated variance of error  \n",
    "- n = number of observations\n",
    "\n",
    "σ²_hat is usually estimated from the full model (model with all predictors)\n",
    "\n",
    "### Why we use Cp ?\n",
    "- Training error underestimates the real test error  \n",
    "- Cp adds a penalty for model size  \n",
    "- This penalty adjusts the RSS to give better test error estimate  \n",
    "- Cp helps to compare models with different numbers of predictors\n",
    "\n",
    "### How to use Cp ?\n",
    "- Calculate Cp for all models  \n",
    "- The model with the **lowest Cp value** is considered the best  \n",
    "- Because it is expected to have the **lowest test error**\n",
    "\n",
    "### Important points :\n",
    "- Cp balances fit and complexity  \n",
    "- It is used in model selection methods like best subset selection  \n",
    "- Cp works only if σ²_hat is a good estimate of real variance  \n",
    "- Lower Cp means better generalization to unseen data\n",
    "\n",
    "## AIC (Akaike Information Criterion) :\n",
    "AIC is used to select the best model by adjusting the training RSS with a penalty for model complexity.\n",
    "\n",
    "### When to use AIC ?\n",
    "AIC is defined for models fitted by maximum likelihood function.  \n",
    "In case of linear regression with Gaussian errors, maximum likelihood and least squares are the same.  \n",
    "So AIC can be used for linear regression models.\n",
    "\n",
    "### AIC Formula (for least squares model)\n",
    "AIC = (1/n) * [ RSS + 2 * d * σ²_hat ]\n",
    "\n",
    "Where:\n",
    "- RSS = residual sum of squares (training error)  \n",
    "- d = number of predictors used in the model  \n",
    "- σ²_hat = estimate of variance of error  \n",
    "- n = number of observations  \n",
    "\n",
    "Constants are ignored because they don’t affect model comparison.\n",
    "\n",
    "### AIC and Cp relation :\n",
    "For least squares models, AIC and Cp are proportional to each other. \n",
    "Both give the same result in this case.\n",
    "\n",
    "### Why we use AIC ?\n",
    "- Training RSS always decreases as we add more variables  \n",
    "- But test error may increase due to overfitting  \n",
    "- AIC adds a penalty for the number of predictors to control overfitting  \n",
    "- Helps estimate the test error more accurately\n",
    "\n",
    "### How to use AIC ?\n",
    "- Calculate AIC for all models  \n",
    "- The model with the **lowest AIC value** is selected and that is best model.\n",
    "- This model has the best trade-off between fit and complexity\n",
    "\n",
    "### Key points\n",
    "- AIC works when model is fitted by likelihood (like linear regression with normal errors)  \n",
    "- AIC = fit + penalty  \n",
    "- AIC and Cp are equal for least squares models  \n",
    "- Lower AIC means better model\n",
    "\n",
    "## BIC (Bayesian Information Criterion) :\n",
    "BIC is used to select the best model by estimating test error using training RSS with a stronger penalty.\n",
    "\n",
    "### BIC Formula (for least squares model) :\n",
    "BIC = (1/n) * [ RSS + log(n) * d * σ²_hat ]\n",
    "\n",
    "Where:\n",
    "- RSS = residual sum of squares  \n",
    "- d = number of predictors in the model  \n",
    "- n = number of observations  \n",
    "- σ²_hat = estimated variance of error  \n",
    "Constants are ignored as they don't affect comparison\n",
    "\n",
    "### Intuition of BIC :\n",
    "Like Cp and AIC, BIC also balances model fit and complexity  \n",
    "But BIC uses a stronger penalty because log(n) > 2 when n > 7  \n",
    "So BIC prefers smaller models unless additional predictors give large improvement\n",
    "\n",
    "### Why we use BIC ?\n",
    "- Training RSS always decreases with more predictors  \n",
    "- But test error may increase due to overfitting  \n",
    "- BIC penalizes model size more than Cp and AIC  \n",
    "- Helps to select a simpler model that generalizes better\n",
    "\n",
    "### How to use BIC ?\n",
    "- Calculate BIC for each model  \n",
    "- The model with the lowest BIC value is considered the best  \n",
    "- Because it has best trade-off between error and complexity\n",
    "\n",
    "### Comparison with Cp and AIC\n",
    "- Cp and AIC penalty = 2dσ²_hat  \n",
    "- BIC penalty = log(n) * d * σ²_hat  \n",
    "- So BIC gives larger penalty for large n and prefers simpler models\n",
    "\n",
    "## Adjusted R² :\n",
    "\n",
    "Adjusted R² is used to select the best model by adjusting the normal R² with a penalty for unnecessary variables.\n",
    "\n",
    "### R² Recap :\n",
    "\n",
    "R² = 1 − RSS / TSS  \n",
    "RSS = residual sum of squares  \n",
    "TSS = total sum of squares\n",
    "\n",
    "R² always increases when we add more variables, even if they are not useful.  \n",
    "So it is not reliable for model selection.\n",
    "\n",
    "### Adjusted R² Formula : \n",
    "\n",
    "Adjusted R² = 1 − (RSS / (n − d − 1)) / (TSS / (n − 1))\n",
    "\n",
    "Where:\n",
    "- n = number of observations  \n",
    "- d = number of predictors  \n",
    "- RSS = residual sum of squares  \n",
    "- TSS = total sum of squares  \n",
    "\n",
    "### Why we use Adjusted R² ?\n",
    "\n",
    "- Unlike R², it penalizes for adding extra variables  \n",
    "- If the added variable improves the model, adjusted R² increases  \n",
    "- If the added variable is noise, adjusted R² decreases  \n",
    "- So it helps in selecting the correct model size\n",
    "\n",
    "### How to use Adjusted R² ?\n",
    "\n",
    "- Calculate adjusted R² for different models  \n",
    "- The model with the highest adjusted R² is selected  \n",
    "- It gives a good balance between fit and simplicity\n",
    "\n",
    "### Key points\n",
    "\n",
    "- R² always increases with more variables  \n",
    "- Adjusted R² may increase or decrease  \n",
    "- It is easy to compute and useful in practice  \n",
    "- But it has less theoretical justification compared to Cp, AIC, and BIC\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7944d166",
   "metadata": {},
   "source": [
    "## Validation and Cross-Validation\n",
    "\n",
    "These are direct methods to estimate test error. Unlike Cp, AIC, BIC, or adjusted R² which adjust training error mathematically, here we actually test the model on Validation data to get the error.\n",
    "\n",
    "### Validation Set Approach\n",
    "\n",
    "- Split the available data into two parts:\n",
    "  - Training set\n",
    "  - Validation set (also called hold-out set)\n",
    "\n",
    "- Fit the model on training data\n",
    "\n",
    "- Predict on validation set\n",
    "\n",
    "- Calculate validation error (usually MSE) to estimate test error\n",
    "\n",
    "- The model with the lowest validation error is selected\n",
    "\n",
    "### Example\n",
    "\n",
    "In Credit dataset, different models (2-variable to 11-variable) were tested using validation set error (MSE). The quadratic model had lower error than the linear model. Cubic model had slightly higher error than quadratic. So quadratic model was best.\n",
    "\n",
    "### Drawbacks of validation set\n",
    "\n",
    "1. High variability – depends on how data is split\n",
    "2. Wastes data – only part of data is used for training, which weakens the model\n",
    "\n",
    "## Cross-Validation (CV)\n",
    "\n",
    "To solve the above issues, we use CV. It is more stable and uses more data for training.\n",
    "\n",
    "### k-Fold Cross-Validation\n",
    "\n",
    "- Split the data into k equal parts (folds)\n",
    "\n",
    "- Repeat k times:\n",
    "  - One fold = validation\n",
    "  - k-1 folds = training\n",
    "  - Calculate error\n",
    "\n",
    "- Final test error = average of k error values\n",
    "\n",
    "- Usually k = 5 or 10 is used\n",
    "\n",
    "### LOOCV (Leave-One-Out Cross-Validation)\n",
    "\n",
    "- Special case of CV where k = n (every point is used once as validation)\n",
    "\n",
    "- Very accurate but slower for large n\n",
    "\n",
    "- For linear models, a formula exists to compute LOOCV without fitting n models\n",
    "\n",
    "### Benefits of CV\n",
    "\n",
    "- Gives a direct and realistic estimate of test error\n",
    "\n",
    "- Works with any model (linear, logistic, tree, etc.)\n",
    "\n",
    "- Does not require assumptions like error variance or degrees of freedom\n",
    "\n",
    "## One-Standard-Error Rule\n",
    "\n",
    "If multiple models have similar test error, choose the **simplest model** whose error is within 1 standard error of the lowest one.\n",
    "\n",
    "This reduces overfitting and gives a stable, interpretable model.\n",
    "\n",
    "## Comparison with Cp, AIC, BIC, and Adjusted R²\n",
    "\n",
    "| Method         | Type      | Uses real test data? | Needs error variance or d? | Works for all models? |\n",
    "|----------------|-----------|----------------------|-----------------------------|------------------------|\n",
    "| Cp, AIC, BIC   | Indirect  | No                   | Yes                         | No (linear/MLE only)   |\n",
    "| Adjusted R²    | Indirect  | No                   | Yes                         | No (linear only)       |\n",
    "| Validation/CV  | Direct    | Yes                  | No                          | Yes                    |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d02fe116",
   "metadata": {},
   "source": [
    "## Bias-Variance Trade-Off\n",
    "\n",
    "The bias-variance trade-off is a fundamental concept for understanding model performance. \n",
    "\n",
    "- **Bias** is error due to simplifying assumptions made by the model. High bias means the model is too simple and underfits the data.  \n",
    "- **Variance** is error due to model sensitivity to small fluctuations in the training data. High variance means the model overfits the data.\n",
    "\n",
    "A model with low bias and low variance is ideal but usually hard to achieve. Increasing model complexity reduces bias but increases variance. Conversely, simplifying the model increases bias but reduces variance.\n",
    "\n",
    "Regularization techniques such as Ridge, Lasso, and Elastic Net introduce bias by constraining model parameters, but this reduces variance, helping improve generalization. Understanding this trade-off explains why these shrinkage methods are effective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4e78377",
   "metadata": {},
   "source": [
    "# Shrinkage Methods :\n",
    "\n",
    "Shrinkage is used when we want to make our model more stable and less sensitive to noise or correlation in the predictors. Instead of dropping variables like subset selection, shrinkage keeps all predictors but shrinks their coefficients.\n",
    "\n",
    "### Why use shrinkage?\n",
    "\n",
    "- Subset selection chooses only a few predictors, but can be unstable\n",
    "- Small changes in data can give different selected subsets\n",
    "- If predictors are correlated or p is large, model may overfit\n",
    "- Shrinkage reduces model variance and helps avoid overfitting\n",
    "\n",
    "### What is shrinkage?\n",
    "\n",
    "Shrinkage means:\n",
    "- Fit model using all p predictors\n",
    "- But apply a penalty so that coefficient values are pulled closer to 0\n",
    "- This makes the model more stable and generalizes better to test data\n",
    "\n",
    "### Which techniques use shrinkage?\n",
    "\n",
    "There are two main methods:\n",
    "\n",
    "1. **Ridge Regression**\n",
    "   - Shrinks all coefficients but none become exactly zero\n",
    "   - Useful when all predictors are somewhat useful\n",
    "   - Good for multicollinearity\n",
    "\n",
    "2. **Lasso**\n",
    "   - Shrinks some coefficients to exactly zero\n",
    "   - So it also performs variable selection\n",
    "   - Useful when we want a simpler model with fewer predictors\n",
    "\n",
    "## 1. Ridge Regression\n",
    "\n",
    "Ridge is a shrinkage method where we fit all predictors but add a penalty on the size of the coefficients. It helps in reducing model variance and controlling overfitting, especially when predictors are correlated or $p$ is large.\n",
    "\n",
    "### Why use Ridge?\n",
    "\n",
    "- Least squares works well only when $n \\gg p$ and predictors are not highly correlated\n",
    "- If predictors are correlated or model is too flexible, variance becomes high\n",
    "- Ridge controls this by shrinking coefficients closer to zero\n",
    "- It keeps all variables in the model but makes their influence smaller\n",
    "\n",
    "### Ridge Regression Formula\n",
    "\n",
    "Ridge minimizes this function:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "- First term = usual least squares (fit the data)\n",
    "- Second term = shrinkage penalty (regularization)\n",
    "- $\\lambda$ is a tuning parameter $\\geq 0$\n",
    "\n",
    "### What lambda ($\\lambda$) does\n",
    "\n",
    "- $\\lambda = 0$ → Ridge = OLS (no penalty)\n",
    "- $\\lambda > 0$ → coefficients are pushed closer to 0\n",
    "- $\\lambda \\rightarrow \\infty$ → all coefficients go to 0\n",
    "- Larger $\\lambda$ means stronger penalty\n",
    "\n",
    "We choose the best $\\lambda$ using cross-validation.\n",
    "\n",
    "### What is the penalty term?\n",
    "\n",
    "Penalty term:  \n",
    "$$\n",
    "\\lambda (\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_p^2)\n",
    "$$\n",
    "\n",
    "- This term discourages large values of coefficients\n",
    "- It is added to the RSS, so the optimizer prefers smaller $\\beta$ values\n",
    "- Helps in reducing overfitting and model complexity\n",
    "\n",
    "### Why is $\\lambda$ a penalty even though it's positive?\n",
    "\n",
    "Because we are minimizing:\n",
    "\n",
    "$$\n",
    "\\text{RSS} + \\lambda \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "- Adding $\\lambda \\sum \\beta^2$ increases total loss\n",
    "- So the model avoids large coefficients\n",
    "- That’s why it acts as a penalty even though $\\lambda$ is positive\n",
    "\n",
    "### What is L2 regularization?\n",
    "\n",
    "- The term \"L2\" comes from the L2 norm of the coefficient vector:\n",
    "\n",
    "$$\n",
    "||\\beta||_2 = \\sqrt{\\beta_1^2 + \\beta_2^2 + \\dots + \\beta_p^2}\n",
    "$$\n",
    "\n",
    "- In ridge, we use the **squared L2 norm** as the penalty\n",
    "- So ridge regression is also called **L2 regularization**\n",
    "- It limits the total \"size\" of coefficients without making any of them exactly zero\n",
    "\n",
    "### Centering the data before Ridge\n",
    "\n",
    "Before applying ridge, we must center the predictors:\n",
    "\n",
    "$$\n",
    "x_j^{\\text{centered}} = x_j - \\bar{x}_j\n",
    "$$\n",
    "\n",
    "- Intercept $\\beta_0$ is not penalized\n",
    "- Centering ensures $\\beta_0$ only captures the mean of $y$\n",
    "- Most libraries like sklearn do this internally\n",
    "\n",
    "### What happens if we don’t center?\n",
    "\n",
    "- The intercept gets mixed with penalized coefficients\n",
    "- Penalty behaves incorrectly\n",
    "- Model can become biased or unstable\n",
    "\n",
    "### Summary of Ridge Regression\n",
    "\n",
    "- Adds penalty on large coefficients\n",
    "- Keeps all predictors but shrinks their influence\n",
    "- Doesn’t set any coefficient to zero\n",
    "- $\\lambda$ controls how strong the shrinkage is\n",
    "- Choose $\\lambda$ using cross-validation\n",
    "- Always center the data before applying ridge\n",
    "- Ridge is also called **L2 Regularization**, because it adds the squared L2 norm of coefficients as a penalty\n",
    "\n",
    "## Ridge Regression – Application to the Credit Data\n",
    "\n",
    "This example shows how ridge regression behaves on the Credit dataset with 10 predictors.\n",
    "\n",
    "### What happens to coefficients as λ increases?\n",
    "\n",
    "We fit ridge regression with different λ values and observe:\n",
    "\n",
    "- Each predictor’s coefficient is plotted as a function of λ\n",
    "- When λ = 0 → Ridge = OLS → coefficients are same as least squares\n",
    "- As λ increases → coefficients shrink towards 0\n",
    "- When λ is very large → all coefficients become almost 0 → this is like a null model with no predictors\n",
    "\n",
    "Some variables like **income**, **limit**, **rating**, and **student** have the largest initial coefficients. These are shown in color in the plot.\n",
    "\n",
    "While most coefficients shrink smoothly, some (like **rating**) may increase slightly at certain λ values. But overall, the **total size of all coefficients decreases** as λ increases.\n",
    "\n",
    "### Plot with L2 Norm Ratio\n",
    "\n",
    "Instead of showing λ on the x-axis, we can also plot:\n",
    "\n",
    "$$\n",
    "\\frac{||\\hat{\\beta}_\\lambda||_2}{||\\hat{\\beta}_{\\text{OLS}}||_2}\n",
    "$$\n",
    "\n",
    "- This ratio tells us **how much the ridge coefficients have shrunk compared to OLS**\n",
    "- At λ = 0 → ratio = 1 → no shrinkage\n",
    "- At large λ → ratio → 0 → all coefficients are almost zero\n",
    "\n",
    "This helps us understand the **overall shrinkage effect** in a single number.\n",
    "\n",
    "### Ridge is not scale-invariant\n",
    "\n",
    "- In OLS: If we multiply a variable (e.g. income) by 1000, the coefficient gets divided by 1000 → no effect on model\n",
    "- In Ridge: Because we penalize squares of coefficients, large-scale variables get **penalized more**\n",
    "\n",
    "This means ridge regression is **sensitive to how each variable is scaled**.\n",
    "\n",
    "### Solution: Standardize the predictors\n",
    "\n",
    "To fix this, we **standardize** each predictor before fitting ridge:\n",
    "\n",
    "$$\n",
    "\\tilde{x}_{ij} = \\frac{x_{ij} - \\bar{x}_j}{\\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (x_{ij} - \\bar{x}_j)^2}}\n",
    "$$\n",
    "\n",
    "After standardizing:\n",
    "\n",
    "- Each predictor has mean = 0 and std deviation = 1\n",
    "- All variables are on the same scale\n",
    "- Ridge now treats all predictors equally\n",
    "\n",
    "The coefficients plotted in this example are based on **standardized predictors**.\n",
    "\n",
    "### Summary of Ridge Regression on Credit Data\n",
    "\n",
    "- As λ increases, coefficients shrink towards 0\n",
    "- L2 norm ratio shows total shrinkage amount\n",
    "- Ridge is **not scale-invariant**\n",
    "- Always **standardize** predictors before applying ridge\n",
    "- This example shows how ridge controls model complexity and handles multicollinearity\n",
    "\n",
    "## Why Ridge Regression is Better than OLS\n",
    "\n",
    "- Ridge regression improves over OLS by balancing bias and variance.\n",
    "- OLS has low bias but very high variance, especially when number of predictors (p) is close to or larger than number of observations (n).\n",
    "- Ridge adds a penalty ($\\lambda$) to shrink coefficients and reduce model flexibility.\n",
    "- As lambda increases:\n",
    "  - Coefficients get smaller\n",
    "  - Variance decreases\n",
    "  - Bias increases slightly\n",
    "- Ridge works well because reducing variance helps more than the small increase in bias.\n",
    "- In some cases, OLS can perform as bad as a null model (when $\\lambda$ is very high), but ridge with a good $\\lambda$ gives much lower test error.\n",
    "- OLS fails completely when p > n (no unique solution), but ridge still works fine by shrinking coefficients.\n",
    "- Ridge is also much faster to compute than subset selection, which tries all possible models.\n",
    "- Overall, ridge gives more stable, generalizable, and efficient models than OLS when predictors are many or correlated.\n",
    "\n",
    "## 2. Lasso Regression\n",
    "\n",
    "Lasso stands for **Least Absolute Shrinkage and Selection Operator**.  \n",
    "It is a regularization method like Ridge, but it solves one main problem that Ridge cannot. Ridge regression shrinks coefficients but never sets any of them to zero, so it always keeps all variables in the model. Lasso, on the other hand, can shrink some coefficients exactly to zero. This means it does both shrinkage and variable selection.\n",
    "\n",
    "### Why use Lasso?\n",
    "\n",
    "- Ridge always keeps all variables, even if they are not important\n",
    "- Lasso gives simpler and more interpretable models by removing unimportant predictors\n",
    "- It is useful when we want to know which features actually matter\n",
    "\n",
    "### Lasso Loss Function\n",
    "\n",
    "Lasso minimizes this function:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2 + \\lambda \\sum_{j=1}^{p} |\\beta_j|\n",
    "$$\n",
    "\n",
    "- First term is the usual least squares error (RSS)\n",
    "- Second term is the L1 penalty (sum of absolute values of coefficients)\n",
    "- λ is the tuning parameter\n",
    "\n",
    "### L1 vs L2\n",
    "\n",
    "- L1 norm = $|\\beta_1| + |\\beta_2| + ... + |\\beta_p|$\n",
    "- L2 norm = $\\sqrt{\\beta_1^2 + \\beta_2^2 + ... + \\beta_p^2}$\n",
    "\n",
    "L1 norm has sharp corners in its geometry. So when the model tries to minimize the loss, it naturally sets some coefficients to exactly zero. This is why Lasso performs variable selection.\n",
    "\n",
    "### What does Lasso do?\n",
    "\n",
    "- Shrinks coefficients like Ridge\n",
    "- But also sets some coefficients exactly to 0\n",
    "- Gives sparse models (only a few variables used)\n",
    "- More interpretable when p is large\n",
    "\n",
    "### Example from Credit Dataset\n",
    "\n",
    "- When λ = 0 → Lasso = OLS → all variables used\n",
    "- As λ increases → Lasso removes unimportant variables\n",
    "- In the Credit data:\n",
    "  - **rating** enters first\n",
    "  - Then **student**, **limit**, and **income**\n",
    "  - Other variables are only included if λ is small\n",
    "\n",
    "So based on λ, Lasso can give models with 1, 2, 3, ... or all 10 variables. This makes it flexible and useful.\n",
    "\n",
    "### Summary of Lasso\n",
    "\n",
    "- Full form: **Least Absolute Shrinkage and Selection Operator**\n",
    "- Adds L1 penalty to the loss\n",
    "- Can shrink some coefficients to 0\n",
    "- Performs both shrinkage and variable selection\n",
    "- Produces simpler, more interpretable models\n",
    "- Works best when some variables are not important\n",
    "- Choose λ using cross-validation\n",
    "\n",
    "## Another Form of Ridge and Lasso Regression\n",
    "\n",
    "Ridge and Lasso can also be written in another way. Instead of adding a penalty, we can write them as constraint problems with a fixed budget.\n",
    "\n",
    "This is just another way to write the same idea. Both forms give the same solution for the right value of λ or s.\n",
    "\n",
    "### Lasso Constraint Form\n",
    "\n",
    "Minimize RSS subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{p} |\\beta_j| \\leq s\n",
    "$$\n",
    "\n",
    "This means: Find the best model that keeps the total absolute size of coefficients within a fixed budget s.\n",
    "\n",
    "When s is large → gives the least squares solution  \n",
    "When s is small → forces some coefficients to 0\n",
    "\n",
    "### Ridge Constraint Form\n",
    "\n",
    "Minimize RSS subject to:\n",
    "\n",
    "$$\n",
    "\\sum_{j=1}^{p} \\beta_j^2 \\leq s\n",
    "$$\n",
    "\n",
    "This means: Find the best model that keeps the total squared size of coefficients within the budget s.\n",
    "\n",
    "As s gets smaller → coefficients shrink more  \n",
    "Ridge will never set coefficients exactly to 0\n",
    "\n",
    "### Geometry when p = 2\n",
    "\n",
    "Lasso: constraint region is a diamond  \n",
    "→ sharp corners → leads to sparse models (some β = 0)\n",
    "\n",
    "Ridge: constraint region is a circle  \n",
    "→ smooth boundary → all β are small but non-zero\n",
    "\n",
    "### Connection to Best Subset Selection\n",
    "\n",
    "Best subset selection can also be written as:\n",
    "\n",
    "$$\n",
    "\\sum I(\\beta_j \\neq 0) \\leq s\n",
    "$$\n",
    "\n",
    "This means: choose the best model using at most s predictors  \n",
    "I() = 1 if the coefficient is non-zero, otherwise 0\n",
    "\n",
    "It gives best interpretation but is computationally slow when p is large\n",
    "\n",
    "### Comparison of All Three\n",
    "\n",
    "Ridge uses squared penalty → shrinks coefficients  \n",
    "Lasso uses absolute value penalty → shrinks and selects  \n",
    "Best subset selection limits the number of predictors directly\n",
    "\n",
    "Lasso is a good middle point. It does feature selection like subset but is fast like ridge\n",
    "\n",
    "### Summary\n",
    "\n",
    "Lasso and Ridge can be written with constraints instead of penalties  \n",
    "Lasso: L1 constraint → forces some β to 0  \n",
    "Ridge: L2 constraint → shrinks all β but none go to 0  \n",
    "Subset: direct limit on number of variables  \n",
    "Lasso gives sparse models with better interpretation and good computation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52b80395",
   "metadata": {},
   "source": [
    "## Why Lasso Sets Some Coefficients to Zero but Ridge Does Not\n",
    "\n",
    "We already saw that Ridge and Lasso both shrink coefficients using a constraint on their size. But only Lasso sets some coefficients exactly to 0. This happens because of the shape of their constraint regions.\n",
    "\n",
    "### Understanding with Geometry\n",
    "\n",
    "Both models try to minimize RSS (error), but they are restricted to stay within a certain region.\n",
    "\n",
    "- The least squares solution is at the center of ellipses (error contours)\n",
    "- The model stops where the first ellipse touches the constraint boundary\n",
    "\n",
    "In the image below:\n",
    "\n",
    "- **Left side** shows Lasso (diamond constraint)\n",
    "- **Right side** shows Ridge (circular constraint)\n",
    "\n",
    "At the point of contact:\n",
    "- Lasso can hit a **corner**, which means one coefficient becomes **exactly zero**\n",
    "- Ridge hits a **smooth edge**, so all coefficients are **non-zero**\n",
    "\n",
    "This is the reason Lasso can **remove features** (feature selection), and Ridge only **shrinks them**\n",
    "\n",
    "![Lasso vs Ridge Contours](lasso_ridge.png)\n",
    "\n",
    "### When p > 2\n",
    "\n",
    "- Ridge becomes a sphere or hypersphere\n",
    "- Lasso becomes a polyhedron or polytope\n",
    "- Lasso still has corners → gives sparse solutions\n",
    "- Ridge stays smooth → no zero coefficients\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Lasso uses L1 constraint → gives sharp corners → some β = 0\n",
    "- Ridge uses L2 constraint → smooth boundary → all β ≠ 0\n",
    "- Lasso does automatic variable selection, Ridge only shrinks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9313e2a0",
   "metadata": {},
   "source": [
    "## Elastic Net Regression\n",
    "\n",
    "### What is Elastic Net?\n",
    "\n",
    "Elastic Net is a regression technique that combines both Lasso (L1) and Ridge (L2) penalties.  \n",
    "It is useful when:\n",
    "\n",
    "- Features are correlated\n",
    "- Some features are irrelevant\n",
    "- We want both regularization and feature selection\n",
    "\n",
    "### Elastic Net Objective Function (2 Forms)\n",
    "\n",
    "#### 1. Separate penalties:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{RSS} + \\lambda_1 \\sum_{j=1}^{p} |\\beta_j| + \\lambda_2 \\sum_{j=1}^{p} \\beta_j^2\n",
    "$$\n",
    "\n",
    "#### 2. Combined λ with mixing parameter α:\n",
    "\n",
    "$$\n",
    "\\text{Loss} = \\text{RSS} + \\lambda \\left[ \\alpha \\sum_{j=1}^{p} |\\beta_j| + (1 - \\alpha) \\sum_{j=1}^{p} \\beta_j^2 \\right]\n",
    "$$\n",
    "\n",
    "- $\\lambda$ : total regularization strength  \n",
    "- $\\alpha$ in [0, 1] : controls mixing between L1 and L2  \n",
    "  - $\\alpha$ = 1 : Lasso  \n",
    "  - $\\alpha$ = 0 : Ridge  \n",
    "  - 0 < $\\alpha$ < 1 : Elastic Net\n",
    "\n",
    "### What Elastic Net Does\n",
    "\n",
    "- Shrinks coefficients like Ridge\n",
    "- Sets some coefficients to zero like Lasso\n",
    "- Performs both variable selection and regularization\n",
    "- Works better than Lasso when predictors are correlated\n",
    "\n",
    "### Why Use Elastic Net?\n",
    "\n",
    "- Lasso may randomly drop one of the correlated variables\n",
    "- Ridge keeps all variables but can’t remove unimportant ones\n",
    "- Elastic Net handles both situations:\n",
    "  - Keeps useful correlated variables\n",
    "  - Removes irrelevant ones\n",
    "\n",
    "### Elastic Net vs Ridge vs Lasso\n",
    "\n",
    "| Method        | Penalty Type | Feature Selection | Handles Correlation |\n",
    "|---------------|--------------|-------------------|---------------------|\n",
    "| Ridge         | L2           | No                | Yes                 |\n",
    "| Lasso         | L1           | Yes               | No (fails when corr)|\n",
    "| Elastic Net   | L1 + L2      | Yes               | Yes                 |\n",
    "\n",
    "### Real-life Example\n",
    "\n",
    "If **area** and **number_of_rooms** are highly correlated:\n",
    "\n",
    "- Lasso may drop one randomly\n",
    "- Ridge keeps both but doesn’t remove others\n",
    "- Elastic Net may keep both if needed and remove the rest\n",
    "\n",
    "### Hyperparameters\n",
    "\n",
    "- $\\lambda$ : controls penalty size (higher = more shrinkage)\n",
    "- $\\alpha$: controls L1 vs L2 balance\n",
    "\n",
    "Use **cross-validation** to choose best values for $\\lambda$ and $\\alpha$\n",
    "\n",
    "### Summary\n",
    "\n",
    "- Elastic Net = Lasso + Ridge\n",
    "- Works well with correlated features\n",
    "- Gives sparse and stable models\n",
    "- Helps avoid overfitting and improves interpretation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41242a89",
   "metadata": {},
   "source": [
    "# Selecting the Tuning Hyperparameter (λ) Using Cross-Validation/ Grid Search\n",
    "\n",
    "When using **ridge regression** or **lasso**, choosing the right tuning parameter **λ** (lambda) is very important. This parameter controls how much the model coefficients are shrunk towards zero, which affects the model's accuracy and complexity.\n",
    "\n",
    "## What is the tuning parameter λ?\n",
    "\n",
    "- **λ controls shrinkage:**\n",
    "  - A **large λ** means strong shrinkage → coefficients get closer to zero → simpler model.\n",
    "  - A **small λ** means little shrinkage → model is similar to least squares → more complex.\n",
    "- Choosing λ balances the trade-off between **bias and variance**.\n",
    "\n",
    "## Why do we use cross-validation to select λ?\n",
    "\n",
    "- We want to find the λ that results in the best prediction accuracy on new data.\n",
    "- Cross-validation (CV) helps estimate the model’s prediction error on unseen data.\n",
    "- By comparing CV errors for different λ values, we pick the one that minimizes the error.\n",
    "\n",
    "## Step-by-step process to select λ with Cross-Validation:\n",
    "\n",
    "1. **Choose a set (grid) of candidate λ values.**  \n",
    "   For example, a sequence from very small (close to 0) to large values.\n",
    "\n",
    "2. **For each λ value, fit the model on training data.**\n",
    "\n",
    "3. **Perform cross-validation:**  \n",
    "   - Split data into folds (e.g., 10-fold CV).  \n",
    "   - For each fold:  \n",
    "     - Fit the model on the training folds.  \n",
    "     - Calculate prediction error on the validation fold.  \n",
    "   - Average these errors to get the CV error for that λ.\n",
    "\n",
    "4. **Select the λ with the smallest average CV error.**\n",
    "\n",
    "5. **Refit the model on the entire dataset using the selected λ to get the final model.**\n",
    "\n",
    "## What do the examples tell us?\n",
    "\n",
    "- In ridge regression on Credit data (Figure 6.12), the optimal λ from leave-one-out CV was small, meaning only a little shrinkage was needed.  \n",
    "- The CV error curve was flat near the minimum, showing many λ values work similarly well.  \n",
    "- In such cases, using the least squares model (λ close to zero) may be fine.\n",
    "\n",
    "- In the lasso example (Figure 6.13), ten-fold CV helped pick a λ that gave zero coefficients for noise variables and non-zero for true predictors (signal variables).  \n",
    "- This shows lasso + CV can successfully identify important variables even with few observations and many predictors.\n",
    "\n",
    "## Important points to remember:\n",
    "\n",
    "- If the CV error curve is **flat around the minimum λ**, many values perform similarly.  \n",
    "  You can pick the simplest model or even the least squares solution.\n",
    "\n",
    "- **Types of CV:**  \n",
    "  - Leave-One-Out CV (LOOCV): more accurate but slower.  \n",
    "  - K-Fold CV (e.g., 10-fold): common and faster, good balance.\n",
    "\n",
    "- For **small or high-dimensional datasets**, CV is especially useful to avoid overfitting.\n",
    "\n",
    "- After selecting λ, always **refit the model on the entire dataset** using that λ.\n",
    "\n",
    "- Cross-validation gives a good estimate but results can vary; consider combining it with domain knowledge.\n",
    "\n",
    "## Summary Table\n",
    "\n",
    "| Step               | What to do                                  |\n",
    "|--------------------|--------------------------------------------|\n",
    "| 1. Select λ grid   | Choose a range of λ values to try            |\n",
    "| 2. Fit models      | Fit model on training folds for each λ       |\n",
    "| 3. Compute CV error | Calculate average validation error per λ     |\n",
    "| 4. Choose λ        | Pick λ with the lowest average CV error      |\n",
    "| 5. Refit model     | Fit final model on all data with chosen λ    |\n",
    "\n",
    "This method ensures that the chosen tuning parameter λ leads to the best model performance on new, unseen data by using cross-validation error as a reliable guide.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5440261",
   "metadata": {},
   "source": [
    "## 3. Dimension Reduction Methods\n",
    "\n",
    "### Why Dimension Reduction?\n",
    "\n",
    "When the number of predictors (**p**) is large:\n",
    "\n",
    "- The model becomes complex  \n",
    "- Variance increases  \n",
    "- Risk of **overfitting** becomes high\n",
    "\n",
    "Earlier methods like **Subset Selection** and **Shrinkage (Ridge/Lasso)** handled this by:\n",
    "\n",
    "- Using a **subset** of predictors  \n",
    "- Or **shrinking** the coefficients  \n",
    "\n",
    "But both methods work **using the original variables**. Now, we look at a different approach: **transforming the variables** before fitting the model.\n",
    "\n",
    "### What is Dimension Reduction?\n",
    "\n",
    "Instead of using the original predictors $X_1, X_2, \\ldots, X_p$, we create new predictors $Z_1, Z_2, \\ldots, Z_M$ where $M < p$.\n",
    "\n",
    "Each $Z_m$ is a **linear combination** of the original predictors:\n",
    "\n",
    "$$\n",
    "Z_m = \\sum_{j=1}^{p} \\phi_{jm} X_j\n",
    "$$\n",
    "\n",
    "Then, we fit a linear model:\n",
    "\n",
    "$$\n",
    "y_i = \\theta_0 + \\sum_{m=1}^{M} \\theta_m Z_{im} + \\epsilon_i\n",
    "$$\n",
    "\n",
    "This is still a linear model, but now with **M transformed variables** instead of $p$.\n",
    "\n",
    "### Why This Helps?\n",
    "\n",
    "We are estimating **fewer coefficients**: From $p + 1$ (in original regression) to $M + 1$ (in transformed model). This helps:\n",
    "\n",
    "- Reduce **variance**  \n",
    "- Keep only the most important components  \n",
    "- Avoid overfitting\n",
    "\n",
    "### Connection to Original Model\n",
    "\n",
    "From the transformation:\n",
    "\n",
    "$$\n",
    "Z_m = \\sum_{j=1}^{p} \\phi_{jm} X_j\n",
    "$$\n",
    "\n",
    "So the model becomes:\n",
    "\n",
    "$$\n",
    "\\sum_{m=1}^{M} \\theta_m Z_m = \\sum_{j=1}^{p} \\beta_j X_j\n",
    "$$\n",
    "\n",
    "where\n",
    "\n",
    "$$\n",
    "\\beta_j = \\sum_{m=1}^{M} \\theta_m \\phi_{jm}\n",
    "$$\n",
    "\n",
    "This means: The dimension reduction model is a **special case** of linear regression where coefficients $\\beta_j$ are **constrained** to take a certain form.\n",
    "\n",
    "### What Happens When $M = p$?\n",
    "\n",
    "If:\n",
    "\n",
    "- We take $M = p$  \n",
    "- And the $Z_m$'s are linearly independent  \n",
    "\n",
    "Then: No reduction happens, and this model is the same as original least squares. So, **dimension reduction only occurs when $M < p$**.\n",
    "\n",
    "### Two-Step Process\n",
    "\n",
    "Every dimension reduction method follows 2 main steps:\n",
    "\n",
    "1. **Create transformed predictors** $Z_1, Z_2, \\ldots, Z_M$  \n",
    "2. **Fit linear regression** on these new predictors  \n",
    "\n",
    "The challenge is: How do we choose the combinations (i.e., the $\\phi_{jm}$'s)?  \n",
    "\n",
    "There are different methods for that. In this chapter, we look at:  \n",
    "\n",
    "- **Principal Component Regression (PCR)**  \n",
    "- **Partial Least Squares (PLS)**  \n",
    "\n",
    "### Summary\n",
    "\n",
    "- **Dimension Reduction** transforms original features into new combinations  \n",
    "- Reduces problem size from $p+1$ to $M+1$  \n",
    "- Helps in **high-dimensional** settings and **correlated variables**  \n",
    "- Still uses **linear regression** but with transformed predictors  \n",
    "- Introduces **bias** but reduces **variance**  \n",
    "- Two main methods: **PCR** and **PLS**  \n",
    "\n",
    "## a. Principal Component Regression (PCR) :\n",
    "## What is PCR?\n",
    "Principal Components Regression (PCR) is a two-step process:\n",
    "1. First, we use Principal Component Analysis (PCA) to reduce the number of input variables.\n",
    "2. Then, we use linear regression on these principal components instead of the original variables.\n",
    "\n",
    "PCR is useful when there are many predictors that are highly correlated or when we want to avoid overfitting.\n",
    "\n",
    "## Steps of PCR\n",
    "\n",
    "### Step 1: Standardize the data\n",
    "Standardize each variable so that all have mean zero and standard deviation one.\n",
    "\n",
    "$$\n",
    "X_j^{scaled} = \\frac{X_j - \\bar{X}_j}{s_j}\n",
    "$$\n",
    "\n",
    "### Step 2: Perform PCA on the predictors\n",
    "PCA finds new variables $Z_1, Z_2, \\dots, Z_p$ that are linear combinations of the original variables:\n",
    "\n",
    "$$\n",
    "Z_1 = \\phi_{11}(X_1 - \\bar{X}_1) + \\phi_{21}(X_2 - \\bar{X}_2) + \\dots + \\phi_{p1}(X_p - \\bar{X}_p)\n",
    "$$\n",
    "\n",
    "These $Z$ variables are called principal components. The first component captures the most variation.\n",
    "\n",
    "### Step 3: Select top M components\n",
    "We select the top M components that capture most of the variation. The value of M is usually chosen by cross-validation.\n",
    "\n",
    "### Step 4: Run linear regression on the selected components\n",
    "\n",
    "$$\n",
    "Y = \\beta_0 + \\beta_1 Z_1 + \\beta_2 Z_2 + \\dots + \\beta_M Z_M + \\varepsilon\n",
    "$$\n",
    "\n",
    "## Important Notes\n",
    "- PCR reduces dimension but does not select specific features.\n",
    "- Each component is a combination of all original features.\n",
    "- PCR does not perform as well when the response depends on variables that have small variance.\n",
    "- Always standardize the data before applying PCA.\n",
    "- Choose number of components M using cross-validation.\n",
    "\n",
    "## When to Use PCR\n",
    "- When predictors are highly correlated\n",
    "- When number of predictors is large compared to number of observations\n",
    "- When we want to avoid overfitting by reducing model complexity\n",
    "\n",
    "## b. Partial Least Squares (PLS)\n",
    "\n",
    "PLS is a dimension reduction method that helps us when we have many predictors and we want to predict a response variable (Y). It works in a similar way to PCR but with one important difference.\n",
    "\n",
    "### Key Point\n",
    "PLS is **supervised**, while PCR is **unsupervised**.\n",
    "\n",
    "### What PLS Does\n",
    "- PLS creates new variables Z₁, Z₂, ..., Zₘ which are linear combinations of original predictors X₁, X₂, ..., Xₚ.\n",
    "- These new variables are used in linear regression to predict Y.\n",
    "- But when creating Zs, PLS uses Y to guide the process. So the new components are not just good at summarizing X but also good for predicting Y.\n",
    "\n",
    "### How PLS Works (Step by Step)\n",
    "1. Standardize all predictors X.\n",
    "2. For each predictor Xⱼ, do a simple linear regression of Y on Xⱼ.\n",
    "3. Use the slope (coefficient) from that regression as a weight φⱼ₁.\n",
    "   - These weights are proportional to the correlation between Xⱼ and Y.\n",
    "4. Use these weights to create Z₁:\n",
    "   \n",
    "   Z₁ = φ₁₁·X₁ + φ₂₁·X₂ + ... + φₚ₁·Xₚ\n",
    "\n",
    "5. Z₁ gives more weight to predictors that are more related to Y.\n",
    "6. Then adjust all predictors and Y by removing the effect of Z₁ (take residuals).\n",
    "7. From these residuals, compute Z₂ in the same way.\n",
    "8. Repeat this to get as many components as needed.\n",
    "\n",
    "### Choosing Number of Components\n",
    "- The number of components M is a tuning parameter.\n",
    "- We choose M using cross-validation to get best prediction performance.\n",
    "\n",
    "### Important Notes\n",
    "- PLS focuses on both explaining X and predicting Y.\n",
    "- PCR only focuses on X (ignores Y), so it may miss important relationships.\n",
    "- PLS often performs better than PCR in prediction tasks.\n",
    "- But, since PLS is supervised, it may also increase variance, so it's not always guaranteed to be better.\n",
    "\n",
    "### When to Use PLS\n",
    "- You have many predictors.\n",
    "- Predictors are correlated.\n",
    "- You want better prediction of Y.\n",
    "- You want to reduce dimensionality but still keep relationship with Y.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "692d5a7f",
   "metadata": {},
   "source": [
    "# Feature Engineering\n",
    "\n",
    "Feature engineering is the process of creating or modifying input features to improve model performance. It helps make the data more suitable for learning and often leads to better accuracy.\n",
    "\n",
    "We do not change the model — we change the data that goes into the model.\n",
    "\n",
    "### Why use Feature Engineering?\n",
    "\n",
    "* Raw data may not be ready for modeling\n",
    "* Many ML models perform better with clean, informative features\n",
    "* Can help reveal hidden patterns\n",
    "* Improves model performance and generalization\n",
    "* Often more impactful than changing the algorithm\n",
    "\n",
    "### What are common feature engineering techniques?\n",
    "\n",
    "We apply transformations, create new variables, and prepare data so the model can learn better.\n",
    "Some common techniques:\n",
    "\n",
    "## 1. Transforming Features\n",
    "\n",
    "Apply mathematical functions to variables to reduce skewness or handle nonlinearity.\n",
    "\n",
    "* **Log transform**: useful for highly skewed data (e.g., income, price)\n",
    "  **log(x + 1)**\n",
    "* **Square root / square**: used when effect increases nonlinearly\n",
    "  **sqrt(x)**, **x²**\n",
    "* **Inverse**: when large values should have smaller effect\n",
    "  **1 / x**\n",
    "\n",
    "## 2. Creating Interaction Terms\n",
    "\n",
    "Combine two or more features to capture interactions between them.\n",
    "\n",
    "* Example:\n",
    "  **age × income**\n",
    "  **bedrooms × area**\n",
    "\n",
    "* Helps linear models learn more complex relationships\n",
    "\n",
    "## 3. Encoding Categorical Variables\n",
    "\n",
    "Convert categories to numbers so models can use them.\n",
    "\n",
    "* **One-Hot Encoding**\n",
    "  Creates binary columns for each category\n",
    "  Example: **color\\_red**, **color\\_blue**\n",
    "  Used in linear models, logistic regression, etc.\n",
    "\n",
    "* **Label Encoding**\n",
    "  Assigns an integer to each category\n",
    "  Example: Red = 0, Blue = 1\n",
    "  Suitable for tree-based models\n",
    "\n",
    "## 4. Binning (Discretization)\n",
    "\n",
    "Convert continuous values into categories.\n",
    "\n",
    "* Example:\n",
    "  Age → **\\[0–18]**, **\\[19–60]**, **60+**\n",
    "  Income → **low**, **medium**, **high**\n",
    "\n",
    "* Reduces noise and helps detect threshold-based effects\n",
    "\n",
    "## 5. Handling Missing Values\n",
    "\n",
    "Fill in missing data before modeling.\n",
    "\n",
    "* **Imputation**\n",
    "\n",
    "  * Numerical: use mean or median\n",
    "  * Categorical: use mode (most frequent)\n",
    "\n",
    "* **Missing Indicator**\n",
    "  Add a new feature:\n",
    "  Example: **is\\_missing = 1** if value is missing\n",
    "\n",
    "## 6. Scaling Features\n",
    "\n",
    "Some models (e.g., ridge, lasso, SVM) are sensitive to feature scale.\n",
    "\n",
    "* **Standardization**\n",
    "\n",
    "  **z = (x - mean) / std**\n",
    "\n",
    "* **Min-Max Scaling**\n",
    "  Scales values between 0 and 1\n",
    "\n",
    "* Tree-based models (e.g., Random Forest, XGBoost) do not need scaling\n",
    "\n",
    "## 7. Creating New Features\n",
    "\n",
    "Use domain knowledge to create more informative variables.\n",
    "\n",
    "Examples:\n",
    "\n",
    "* From dates → extract **year**, **month**, **weekday**\n",
    "* From text → **word count**, **length**, **keyword presence**\n",
    "* From location → **distance**, **region**\n",
    "* From transactions → **total spend**, **frequency**\n",
    "\n",
    "New features can reveal patterns not obvious in raw data.\n",
    "\n",
    "### Summary of Feature Engineering\n",
    "\n",
    "* Helps models perform better by improving input data\n",
    "* Includes transformations, encoding, scaling, and feature creation\n",
    "* Should be based on data understanding and model needs\n",
    "* Good features often matter more than the model itself\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d85039",
   "metadata": {},
   "source": [
    "# Conclusion :\n",
    "In this notebook, I have learned how to select the best model and apply regularization techniques such as ridge, lasso, and elastic-net to improve model performance. I gained a deeper understanding of the bias-variance trade-off and how it is influenced by different regularization methods. Additionally, I explored feature engineering and cross-validation techniques and many more things, which are essential for building robust predictive models. Overall, this work enhanced my knowledge of model selection and regularization techniques."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
